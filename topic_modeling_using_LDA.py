# -*- coding: utf-8 -*-
"""topic-modeling-using-LDA

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uA0_xHtyLf6fVq4oerOTT5CS4o2Y2A4b

## 상수 설정
"""

FILE_NAME = "sample_data_60.csv"
FILE_PATH = f"data/{FILE_NAME}"
GROUP_COL = "month"
TARGET_COL = "text"
PROCESSED_COL = "ProcessedData"
HANSPELL_MAX_LENGTH = 500
KONLPY_MAX_LENGTH = 3000

"""## 패키지 설치 및 불러오기

### 전처리 과정에서 필요한 패키지 설치 및 불러오기
"""

import sys
sys.executable

# !pip install py-hanspell

!git clone https://github.com/ssut/py-hanspell.git
!cd /usr/local/lib/python3.8/dist-packages/ && mkdir hanspell
!cd py-hanspell && cp -r hanspell/ /usr/local/lib/python3.8/dist-packages/

!pip install konlpy

import site
import os
import pandas as pd
import re
from tqdm import tqdm
from hanspell import spell_checker
from konlpy.tag import Hannanum

"""### 토픽 모델링 과정에서 필요한 패키지 설치 및 불러오기"""

!pip install pyLDAvis

from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models.ldamodel import LdaModel
from gensim.models import CoherenceModel
import matplotlib.pyplot as plt
from gensim import corpora
import numpy as np
import nltk
nltk.download('punkt')
from sklearn.decomposition import LatentDirichletAllocation
import pyLDAvis.gensim_models 
import gensim

"""## 데이터 및 사전 불러오기 및 사전 중복 검사"""

def duplicate_check(df, dictionary_name):

    src = list(df)[0]
    dest = list(df)[1]
    
    dup = df.duplicated([src])
    df_dup = pd.concat([df, dup], axis=1)
    df_dup.rename(columns={0: 'Dup'}, inplace=True)
    df_dup['Dup'].value_counts()

    if df_dup['Dup'].values.sum() > 0:
        print(f"{dictionary_name}에서 중복 key 값이 발견되었습니다!")
        for i in range(len(df_dup)):
            if df_dup['Dup'][i]:
                # print(f"중복되는 key 값 : {df_dup['#en'][i]} (index at {i})")
                print(f"중복되는 key 값 : {df_dup[src][i]} (index at {i})")

    else:
        print(f"{dictionary_name}에서 중복되는 값이 없습니다.")
    
    print("사전에서 읽어들인 용어의 개수 :", len(df), "\n")

user_dic_df = pd.read_csv("data/dictionary/user_dic_syn.csv", encoding="utf-8", low_memory=False)
stopwords_df = pd.read_csv("data/dictionary/stopwords.csv", encoding="utf-8", low_memory=False)
fix_df = pd.read_csv("data/dictionary/spell_fix.csv", encoding="utf-8", low_memory=False)

duplicate_check(user_dic_df, "user_dic_df")
duplicate_check(stopwords_df, "stopwords_df")
duplicate_check(fix_df, "fix_df")

syn_df = pd.read_csv("data/dictionary/synonym.csv", encoding="utf-8", low_memory=False)

for idx in range(len(syn_df['삭제여부'])):
    if syn_df['삭제여부'][idx]==True:
        syn_df['synonym'][idx] = "" 

# synonym 행 중 값이 없는 행은 삭제
syn_df = syn_df[syn_df['synonym'].notna()] 
syn_df = syn_df[['word', 'synonym']] 

# index 재구성
len_syn = len(syn_df['synonym'])
syn_index = [i for i in range(len_syn)]
syn_df = syn_df.set_index(pd.Index(syn_index))  

duplicate_check(syn_df, "syn_df")

final_fix_df = pd.read_csv("data/dictionary/final_fix.csv", encoding="utf-8", low_memory=False)

final_fix_df = final_fix_df[['problem', 'fix']] 
final_fix_df = final_fix_df[final_fix_df['problem'].notna()] 
final_fix_df = final_fix_df.fillna(' ')

duplicate_check(final_fix_df, "final_fix_df")

try:
  KONLPY_DIC_PATH = '/konlpy/java/data/kE/dic_user.txt'

  correct_path_cnt = 0 # KONLPY_DIC_PATH를 가지고 있는 경로가 1개 뿐이어야 함

  for sitepackages_path in site.getsitepackages():
    if os.path.isfile(sitepackages_path + KONLPY_DIC_PATH) == True:
      correct_path_cnt += 1
      DATA_PATH = sitepackages_path + KONLPY_DIC_PATH

  if correct_path_cnt > 1: # KONLPY_DIC_PATH를 가지고 있는 경로가 중복될 경우
    raise Exception("konlpy 라이브러리를 포함하는 site-packages의 경로가 여러 개 있습니다.\n!pip show konlpy 명령어 실행 후, \"Location:\"에 있는 경로를 직접 입력해주세요. ")

except:
  DATA_PATH = input("\n`!pip show konlpy` 명령어를 실행하여 확인한 \"Location:\"에 있는 경로를 직접 입력해주세요. : ")

dic_user_df = pd.read_table('data/dictionary/dic_user.txt')
dic_user_df.to_csv(DATA_PATH, index=False, header=False, sep='\t')
duplicate_check(dic_user_df, "dic_user.txt")

hannanum = Hannanum()

data = pd.read_csv(FILE_PATH, low_memory=False)
print(f"{FILE_PATH}에서 {len(data)}개의 data를 불러왔습니다.")

data

"""### 데이터의 결측값 여부 확인"""

print(f"총 데이터(row)의 수 : {len(data)}")

print(f"\n===== {TARGET_COL} column의 결측값 여부 확인 =====")
print('결측값 여부 :', data[TARGET_COL].isnull().values.any())
print(f"원본 데이터의 유니크한 값 : {data[TARGET_COL].nunique()}")

print(f"\n===== {GROUP_COL} column의 결측값 여부 확인 =====")
print('결측값 여부 :', data[GROUP_COL].isnull().values.any())
print(f"원본 데이터의 유니크한 값 : {data[GROUP_COL].nunique()}")

"""## 토픽 모델링을 위한 전처리 작업"""

data[PROCESSED_COL] = [False for _ in range(len(data[TARGET_COL]))]

"""### 개행 문자 (`CRLF`) 제거"""

def remove_CRLF(input_str):
    input_str = input_str.replace("\n", " ")
    input_str = input_str.replace("\\n", " ")
    input_str = input_str.replace("\r", " ")
    input_str = input_str.replace("\\r", " ")

    return input_str

for i in range(len(data[TARGET_COL])):
    data[PROCESSED_COL][i] = remove_CRLF(data[TARGET_COL][i])

"""### 이모티콘 문자들, 사이트 URL 주소, 의미없는 문자 지우기"""

def cleansing(input_str):
    output_str = re.sub(pattern='<[U]\+[A-Z0-9]*>', repl=' ', string=input_str)
    output_str = re.sub(pattern='http[s]?://(?:[a-zA-Z]|[0-9]|[_$\-@\.&+:/?=]|[!*\(\),]|(?:%[0-9a-fA-F_][0-9a-fA-F_]))+', repl=' ', string=output_str)
    output_str = re.sub(pattern='^[0-9]+$', repl=' ', string=output_str)

    return output_str

for i in range(len(data[TARGET_COL])):    
    data[PROCESSED_COL][i] = cleansing(data[PROCESSED_COL][i])

"""### 맞춤법 및 띄어쓰기 교정"""

def spell_check(input_str):
    output_str = input_str.replace("&", " ")

    try:
        tmp = spell_checker.check(output_str).checked # 500자가 넘는 경우 빈 문자열로 반환됨
        if tmp!="": 
            output_str = tmp
    except:
        print("hanspell : spell_checker error\n")
    
    return output_str

print("Spell checking start")

for i in tqdm(range(len(data[TARGET_COL]))):
    data[PROCESSED_COL][i] = spell_check(data[PROCESSED_COL][i])

"""#### 교정 오류 수정"""

def spell_fix(spell_checked_str):
    for i in range(len(fix_df["fix"])):
        spell_checked_str = spell_checked_str.replace(fix_df["problem"][i], fix_df["fix"][i])

    return spell_checked_str

for i in tqdm(range(len(data[TARGET_COL]))):
    data[PROCESSED_COL][i] = spell_fix(data[PROCESSED_COL][i])

"""### 데이터의 앞뒤에 공백 추가"""

def add_space(input_str):
    return " "+input_str+" "

for i in range(len(data[TARGET_COL])):
    data[PROCESSED_COL][i] = add_space(data[PROCESSED_COL][i])

"""### 토큰화 전 불용어(stopwords) 제거"""

def remove_stopwords(input_str):
    input_str = re.sub(pattern='([0-9ㄱ-ㅎㅏ-ㅣ]+)', repl='', string=input_str)
    for i in range(len(stopwords_df["nursing"].dropna())):
        input_str = input_str.replace(" "+stopwords_df["nursing"][i]+" ", " ")
    for i in range(len(stopwords_df["korean"].dropna())):
        input_str = input_str.replace(" "+stopwords_df["korean"][i]+" ", " ")
    for i in range(len(stopwords_df["internet"].dropna())):
        input_str = input_str.replace(stopwords_df["internet"][i], " ")
    output_str = re.sub("\s+", " ", input_str)

    return output_str

for i in tqdm(range(len(data[TARGET_COL]))):
    data[PROCESSED_COL][i] = remove_stopwords(data[PROCESSED_COL][i])

"""### 토큰화"""

def tokenization(input_str):
    if input_str.strip() == "": # 문장이 비어있는 경우
        return " "

    elif len(input_str)>KONLPY_MAX_LENGTH: # 문장이 3000자가 넘어갈 때
        # 3000 문자당 자르기
        doc = input_str
        split_list = [doc[i:i+KONLPY_MAX_LENGTH] for i in range(0, len(doc), KONLPY_MAX_LENGTH)]
        
        tmp_list = []
        for phrase in split_list:
            tmp_list = tmp_list + hannanum.nouns(phrase)
            
        temp_list = []
        for word in tmp_list:
            temp_list.append(re.sub(r'[0-9]+', '', word))
        
        return " "+" ".join(temp_list)+" "
        
    else: # 문장이 3000자 미만일 때
        tmp_list = hannanum.nouns(input_str)
        temp_list = []
        for word in tmp_list:
            temp_list.append(re.sub(r'[0-9]+', '', word))

        return " "+" ".join(temp_list)+" "

for i in tqdm(range(len(data[TARGET_COL]))):
    data[PROCESSED_COL][i] = tokenization(data[PROCESSED_COL][i])

"""### 토큰화 후 불용어(stopwords) 제거"""

for i in tqdm(range(len(data[TARGET_COL]))):
    data[PROCESSED_COL][i] = remove_stopwords(data[PROCESSED_COL][i])

"""### 동의어 대체"""

def replace_synonyms(input_str, syn_df):
    TRG_COLUMN = syn_df.columns[0]
    SYN_COLUMN = syn_df.columns[1]

    for i in range(len(syn_df)):
        input_str = input_str.replace(" "+syn_df[TRG_COLUMN][i]+" ", " "+syn_df[SYN_COLUMN][i]+" ")

    return input_str

for i in tqdm(range(len(data[TARGET_COL]))):
    data[PROCESSED_COL][i] = replace_synonyms(data[PROCESSED_COL][i], syn_df)

"""### 한 글자 단어 제거"""

def remove_single_word_from_string(input_str):
    tokened_str_list = input_str.split(' ')
    copy_list = tokened_str_list.copy()
    
    for tk in tokened_str_list:
        if len(tk)<=1:
            copy_list.remove(tk)
    
    return " ".join(copy_list)

for i in range(len(data[TARGET_COL])):
    data[PROCESSED_COL][i] = remove_single_word_from_string(data[PROCESSED_COL][i])

"""### 최종 오류 수정"""

for i in tqdm(range(len(data[TARGET_COL]))):
    data[PROCESSED_COL][i] = replace_synonyms(data[PROCESSED_COL][i], final_fix_df)

"""# 토픽 모델링

## 최적의 토픽 수 계산
"""

def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):
    """
    Compute c_v coherence for various number of topics

    Parameters:
    ----------
    dictionary : Gensim dictionary
    corpus : Gensim corpus
    texts : List of input texts
    limit : Max num of topics

    Returns:
    -------
    model_list : List of LDA topic models
    coherence_values : Coherence values corresponding to the LDA model with respective number of topics
    """
    coherence_values = []
    model_list = []
    for num_topics in range(start, limit, step):
#         model = pyLDAvis.gensim_models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
#         model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)
        model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')
        coherence_values.append(coherencemodel.get_coherence())

    return model_list, coherence_values

def find_optimal_number_of_topics(dictionary, corpus, processed_data, limit=30):  
    start = 1; 
    step = 1; 
    
    model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=processed_data, start=start, limit=limit, step=step) 
    
    x = range(start, limit, step) 
    plt.plot(x, coherence_values) 
    plt.xlabel("Num Topics") 
    plt.ylabel("Coherence score") 
    plt.legend(("coherence_values"), loc='best') 
        
    dic = { coherence_values:x for x, coherence_values in zip(x, coherence_values) }

    plt.text(dic[max(coherence_values)],min(coherence_values),
             str(dic[max(coherence_values)]) + ' at MAX',
             color='r',
             horizontalalignment='center',
             verticalalignment='bottom')
    
    plt.show()
    
    return dic[max(coherence_values)]

# 상위 1,000개의 단어를 보존 
vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000)
X = vectorizer.fit_transform(data[PROCESSED_COL])

# TF-IDF 행렬의 크기 확인
print('TF-IDF 행렬의 크기 :',X.shape)

data["tokenized_text"] = data.apply(lambda row: nltk.word_tokenize(row[PROCESSED_COL]), axis=1)
tokenized_doc = data["tokenized_text"].apply(lambda x: [word for word in x if len(word) > 1])
dictionary = corpora.Dictionary(tokenized_doc)
corpus = [dictionary.doc2bow(text) for text in tokenized_doc]
processed_data = list(np.array(tokenized_doc.tolist()))

limit = 15
NUM_TOPICS = find_optimal_number_of_topics(dictionary, corpus, processed_data, limit)

"""## LDA 모델 생성"""

lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, learning_method='online', random_state=42, max_iter=1)

lda_top = lda_model.fit_transform(X)

# 단어 집합. 1,000개의 단어가 저장됨.
terms = vectorizer.get_feature_names()

def get_topics(components, feature_names, n=10):
    for idx, topic in enumerate(components):
        print("Topic %d:" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])

get_topics(lda_model.components_,terms)

ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)

"""## LDA 모델 시각화"""

pyLDAvis.enable_notebook()
vis = pyLDAvis.gensim_models.prepare(ldamodel, corpus, dictionary)
pyLDAvis.display(vis)

pyLDAvis.save_html(vis, f"results/lda_{NUM_TOPICS}topics.html")

"""## LDA 값 저장

### beta 값 구하기
"""

num_words=50
beta_list = ldamodel.print_topics(num_words)

# print(beta_list)
beta_df = pd.DataFrame(index=range(0,(len(beta_list)*num_words)), columns=['topic', 'term', 'beta'])

# 'topic' 열에 값 집어넣기
for i in range(len(beta_df)):
    beta_df['topic'][i]=int(i/num_words)%num_words

idx_cnt=0
for i in range(len(beta_list)):
#     print(beta_list[i][1].split(" + "))
#     print(len(beta_list))
    for j in range(num_words):
        try:
            beta_df['term'][idx_cnt]=beta_list[i][1].split(" + ")[j].split("*")[1]
            beta_df['beta'][idx_cnt]=beta_list[i][1].split(" + ")[j].split("*")[0]
        except: # topic에 단어가 num_words만큼 있지 않은 경우
            pass
        
        idx_cnt+=1
beta_df.to_csv(f"results/lda_beta_{NUM_TOPICS}topics.csv", mode='w')

"""### gamma 값 구하기"""

# 참고 :
# http://bigdata.emforce.co.kr/wp-content/uploads/%EC%97%A0%ED%8F%AC%EC%8A%A4-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%9E%A9_%ED%86%A0%ED%94%BD%EB%AA%A8%EB%8D%B8%EB%A7%81LDA%EB%B0%A9%EB%B2%95%EB%A1%A0-%EC%A0%95%EB%A6%AC.pdf

def make_topictable_per_doc(ldamodel, corpus):
    topic_table = pd.DataFrame()
    
    for i, topic_list in enumerate(ldamodel[corpus]):
        doc = topic_list[0] if ldamodel.per_word_topics else topic_list
        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)
        
        for j, (topic_num, prop_topic) in enumerate(doc):
            if j==0:
                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic, 4), topic_list]), ignore_index=True)
            else:
                break
    return (topic_table)


topictable = make_topictable_per_doc(ldamodel, corpus)
topictable = topictable.reset_index()
topictable.columns = ['문서번호', '가장 비중이 높은 토픽', '가장 높은 토픽의 비중(gamma)', '각 토픽의 비중']
topictable.to_csv(f"results/lda_gamma_{NUM_TOPICS}topics.csv", mode='w')

